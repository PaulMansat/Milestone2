  container_e02_1580812675067_5020_01_000003
== container_e02_{timestamp}_{app_id}_{attempt_number}_{container_id} 

For each application attempt that failed, i.e., either final status FAILED or exit status non-zero, we are required to print:


========
Complete attempt id (does it include the app id ?)
Location: yarn logs and application log file

========
User
Location: in the yarn log file

========
Start time
Location: in the yarn log file

========
End time
Location: in the yarn log file

========
Container ids with their hostnames for that attempt (sorted by container ids)
Location: in the yarn log file and the application log file

========
Category (see below) for the main error. We are interested in the main error that is the root of the problem. We do not care about subsequent errors that are caused by the main error.
Location: in the app log file: info can be found in logs of different container of the same attempt ! 

First remarks: 
- if error is non-spark related, then in the error report (after the error class message) at java.{something}
- the message starting with: "INFO ApplicationMaster: Final app status: FAILED" interesting to check if error driver related. If rest of message contains "ExecutorLostFailure" => error due exclusively to Executor (and not the driver)
- if we see that error due to driver => have to choose between type 3 or 4 => distinction in the message 

Type 3: Error while executing general (non-spark related) java/scala code at the driver.
How to find ? 
Need to have: INFO ApplicationMaster: Final app status: FAILED + "java.{something}" (maybe also "scala.{something}" -- to be checked)
exemple:
20/04/07 15:57:11 INFO ApplicationMaster: Final app status: FAILED, exitCode: 15, (reason: User class threw exception: java.lang.OutOfMemoryError: GC overhead limit exceeded


Type 4: Error while transferring data between driver and executors.
How to find ? 
has either one of the two lines:
	(i) "Issue communicating with driver in heartbeater" (true for app2)
OR	(ii) INFO ApplicationMaster: Final app status: FAILED + "driver"
exemple: 
20/04/07 15:59:31 INFO ApplicationMaster: Final app status: FAILED, exitCode: 15, (reason: User class threw exception: org.apache.spark.SparkException: Job aborted due to stage failure: Total size of serialized results of 102 tasks (1026.5 MB) is bigger than spark.driver.maxResultSize (1024.0 MB)

Type 5: Error while executing general (non-spark related) java/scala code at the executor.
How to find ? 
	(i) Need to have: INFO ApplicationMaster: Final app status: FAILED + "Executor lost failure"
	(ii) After "java.lang.OutOfMemoryError:" (or any type of error), check if first line related starts with java.{something}, if not, then it is non-spark related 
exemple: 
20/04/07 16:01:58 INFO ApplicationMaster: Final app status: FAILED, exitCode: 15, (reason: User class threw exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 1 times, most recent failure: Lost task 1.0 in stage 0.0 (TID 1, iccluster057.iccluster.epfl.ch, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_e02_1580812675067_5024_01_000002 on host: iccluster057.iccluster.epfl.ch. Exit status: 143. Diagnostics: [2020-04-07 16:01:58.105]Container killed on request. Exit code is 143


Type 6: Error while shuffling data between executors or reading input data into executors.
How to find ? 
We look for two lines: 
	(i) Need to have: INFO ApplicationMaster: Final app status: FAILED + "Executor lost failure"
	(ii) INFO DAGScheduler: Shuffle files lost for executor:
exemple: 
20/04/07 16:19:11 INFO ApplicationMaster: Final app status: FAILED, exitCode: 15, (reason: User class threw exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 2.0 failed 1 times, most recent failure: Lost task 5.0 in stage 2.0 (TID 18, iccluster060.iccluster.epfl.ch, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_e02_1580812675067_5026_01_000002 on host: iccluster060.iccluster.epfl.ch. Exit status: 143. Diagnostics: [2020-04-07 16:19:10.894]Container killed on request. Exit code is 143
[2020-04-07 16:19:10.895]Container exited with a non-zero exit code 143. 


Type 7: Error due to some Spark operation in the driver

Type 8: Error due to some Spark operation in the driver and/or executors.

Type 9: Unknown error/ none of the above

========
Full class name of the main exception/error, if applicable, otherwise “N/A”.
Location: in the application log file
The error message is always after the line: 
ERROR {something}: {something with the word EXCEPTION}
{the error message}


exemples: 
20/04/07 15:26:22 ERROR Executor: Exception in task 2.0 in stage 0.0 (TID 0)
java.lang.OutOfMemoryError: Java heap space

20/04/07 15:28:24 ERROR Utils: Uncaught exception in thread task-result-getter-2
java.lang.OutOfMemoryError: GC overhead limit exceeded

20/04/07 15:57:11 ERROR ApplicationMaster: User class threw exception: java.lang.OutOfMemoryError: GC overhead limit exceeded
java.lang.OutOfMemoryError: GC overhead limit exceeded

20/04/07 15:59:31 ERROR ApplicationMaster: User class threw exception: org.apache.spark.SparkException: Job aborted due to stage failure: Total size of serialized results of 102 tasks (1026.5 MB) is bigger than spark.driver.maxResultSize (1024.0 MB)
org.apache.spark.SparkException: Job aborted due to stage failure: Total size of serialized results of 102 tasks (1026.5 MB) is bigger than spark.driver.maxResultSize (1024.0 MB)

THE ONLY PROBLEM IS WITH APP6, where no error appear ! The only command that seems to be a memory leakage is the following lines: 
20/04/07 16:06:05 INFO ExternalAppendOnlyMap: Thread 65 spilling in-memory map of 413.2 MB to disk (1 time so far)
20/04/07 16:06:32 INFO ExternalAppendOnlyMap: Thread 65 spilling in-memory map of 413.2 MB to disk (2 times so far)
20/04/07 16:06:59 INFO ExternalAppendOnlyMap: Thread 65 spilling in-memory map of 413.2 MB to disk (3 times so far)
20/04/07 16:07:22 INFO ExternalAppendOnlyMap: Thread 65 spilling in-memory map of 413.2 MB to disk (4 times so far)
20/04/07 16:07:49 INFO ExternalAppendOnlyMap: Thread 65 spilling in-memory map of 413.2 MB to disk (5 times so far)
20/04/07 16:08:20 INFO ExternalAppendOnlyMap: Thread 65 spilling in-memory map of 413.2 MB to disk (6 times so far)
20/04/07 16:08:46 INFO ExternalAppendOnlyMap: Thread 65 spilling in-memory map of 413.2 MB to disk (7 times so far)
20/04/07 16:09:14 INFO ExternalAppendOnlyMap: Thread 65 spilling in-memory map of 413.2 MB to disk (8 times so far)
20/04/07 16:09:43 INFO ExternalAppendOnlyMap: Thread 65 spilling in-memory map of 413.2 MB to disk (9 times so far)
20/04/07 16:10:12 INFO ExternalAppendOnlyMap: Thread 65 spilling in-memory map of 413.2 MB to disk (10 times so far)
=> rule: after 10 memory leakage => java.lang.outOfMemoryError ? 

========
Stage at which the application attempt failed, if applicable, otherwise -1
Location: in the application log file 

How to find in the log file: 
INFO DAGScheduler: ResultStage -- not all have latter command
INFO Executor: Executor interrupted -- not all have latter command
INFO YarnClusterScheduler: Stage -- not all have latter command
ERROR TaskSetManager: Task -- not all have latter command
INFO YarnClusterScheduler: Cancelling -- works: full exemple
20/04/07 15:26:22 INFO YarnClusterScheduler: Cancelling stage 0

======== FURTHER INFO NEEDED -- when many lines available, which to take ? => the bigger one ?
Line number inside the application’s source file where the exception was triggered, if it applies, or else -1. If multiple lines are applicable, choose the one that is most relevant to the error.
Location: log application file
How to find ?
Look in the log file for the info {App_name}.scala:{line_number}
exemple of few: 
20/04/07 15:26:22 INFO DAGScheduler: ResultStage 0 (collect at App1.scala:23) failed in 84.447 s due to Job aborted due to stage failure: Task 2 in stage 0.0 failed 1 times, most recent failure: Lost task 2.0 in stage 0.0 (TID 0, iccluster063.iccluster.epfl.ch, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_e02_1580812675067_5020_01_000003 on host: iccluster063.iccluster.epfl.ch. Exit status: 143. Diagnostics: [2020-04-07 15:26:22.670]Container killed on request. Exit code is 143
=> the information we are looking for is: (collect at App1.scala:23)

20/04/07 15:26:22 INFO DAGScheduler: Job 0 failed: collect at App1.scala:23, took 84.506092 s


20/04/07 15:57:11 INFO ApplicationMaster: Unregistering ApplicationMaster with FAILED (diag message: User class threw exception: java.lang.OutOfMemoryError: GC overhead limit exceeded
	at java.lang.Integer.valueOf(Integer.java:832)
	at scala.runtime.BoxesRunTime.boxToInteger(BoxesRunTime.java:65)
	at App3$$anonfun$4$$anonfun$apply$2.apply(App3.scala:22)
	at App3$$anonfun$4$$anonfun$apply$2.apply(App3.scala:22)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.immutable.Range.foreach(Range.scala:160)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at App3$$anonfun$4.apply(App3.scala:22)
	at App3$$anonfun$4.apply(App3.scala:22)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
	at scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:186)
	at App3$.main(App3.scala:22)
	at App3.main(App3.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$4.run(ApplicationMaster.scala:721)
)




org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from iccluster063.iccluster.epfl.ch:37255 in 10 seconds. This timeout is controlled by spark.executor.heartbeatInterval
